{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data originates from the [kaggle copetition](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/description.png\"  width=\"80%\" height=\"20%\">\n",
    "</p>\n",
    "\n",
    "### Dataset Description\n",
    "File descriptions\n",
    " * train.csv - the training set.\n",
    " * test.csv - the test set.\n",
    " * data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here.\n",
    "\n",
    " #### EDA:\n",
    "\n",
    " **Target Variable**\n",
    " \n",
    "The variable we aim to predict is `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import shapiro  \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from plot_tools import plot_distribution, plot_corration_map, PlotRelations\n",
    "from bayesian_opt import Optimizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.space import Integer, Real, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df[['SalePrice']]\n",
    "df_features = df.drop(['Id', 'SalePrice'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine the missing data and input it accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    missing_rate = (X.isnull().sum() / len(X)) * 100\n",
    "    missing_rate = missing_rate.drop(\n",
    "        missing_rate[missing_rate == 0].index).sort_values(ascending=False)[:30]\n",
    "    missing_data = pd.DataFrame({'Missing Ratio': missing_rate})\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = get_missing(df_features)\n",
    "display(missing_data.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alley: Type of alley access to property\n",
    "\n",
    "* NA: \tNo alley access\n",
    "\t\t\n",
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "* NA:\tNo Basement\n",
    "\t\t\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "* NA:\tNo Basement\n",
    "\t\n",
    "BsmtExposure: Refers to walkout or garden level walls\n",
    "\n",
    "* NA:\tNo Basement\n",
    "\t\n",
    "BsmtFinType1: Rating of basement finished area\n",
    "\n",
    "* NA:\tNo Basement\n",
    "\t\t\n",
    "BsmtFinType2: Rating of basement finished area (if multiple types)\n",
    "\n",
    "* NA:\tNo Basement\n",
    "\n",
    "FireplaceQu: Fireplace quality\n",
    "* NA:\tNo Fireplace\n",
    "\t\t\n",
    "GarageType: Garage location\n",
    "* NA:\tNo Garage\n",
    "\t\n",
    "GarageFinish: Interior finish of the garage\n",
    "* NA:\tNo Garage\n",
    "\n",
    "GarageQual: Garage quality\n",
    "* NA:\tNo Garage\n",
    "\t\t\n",
    "GarageCond: Garage condition\n",
    "* NA:\tNo Garage\n",
    "\n",
    "PoolQC: Pool quality\n",
    "* NA:\tNo Pool\n",
    "\t\t\n",
    "Fence: Fence quality\n",
    "* NA:\tNo Fence\n",
    "\t\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "* NA:\tNone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_where_null_is_0 = [\n",
    "                'BsmtQual', 'BsmtCond', 'BsmtExposure', \n",
    "                'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', \n",
    "                'GarageType', 'GarageFinish', 'GarageQual', \n",
    "                'GarageCond', 'PoolQC', 'Fence'\n",
    "            ]\n",
    "values = {\"Functional\": \"Typ\", \n",
    "          \"Alley\": \"None\", \n",
    "          \"MasVnrType\": \"None\", \n",
    "          \"MiscFeature\": \"no_misc_feature\", \n",
    "          **{v:0 for v in variables_where_null_is_0}}\n",
    "df_features.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = get_missing(df_features)\n",
    "display(missing_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = df_features.dtypes[df_features.dtypes != \"object\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.get_dummies(df_features)\n",
    "df_features = df_features.fillna(df_features.mean())\n",
    "missing_data = get_missing(df_features)\n",
    "display(missing_data.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to check the distribution of the target variable and observe its asymmetry. However, for now, we won't perform any transformations. Then we will fit two models: Lasso and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_target['SalePrice'].skew())\n",
    "plot_distribution(df_target, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "y = df_target['SalePrice'].astype(float)\n",
    "X = df_features.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "dict_models = {\n",
    "'lasso': {\n",
    "    'model': Lasso(),\n",
    "    'space': [\n",
    "        Real(0, 0.02, name='alpha'),\n",
    "    ]\n",
    "},\n",
    "\n",
    "'rf': {\n",
    "    'space': [\n",
    "                Integer(100, 1000, name='n_estimators'),\n",
    "                Integer(2, 100, name='min_samples_split'),\n",
    "                Integer(1, 10, name='min_samples_leaf')\n",
    "                ],\n",
    "    'model': RandomForestRegressor()}\n",
    "}\n",
    "\n",
    "for model in dict_models:\n",
    "    model_name = model\n",
    "    space = dict_models[model]['space']\n",
    "    model = dict_models[model]['model']\n",
    "    optimizer = Optimizer(space=space, model=model,\n",
    "                            model_name=model_name, n_calls=20)\n",
    "\n",
    "    optimizer.find_optimal_params(X=X_train, y=y_train)\n",
    "    best_model = optimizer.best_model.fit(X_train, y_train.ravel())\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    plot_rel = PlotRelations(pd.DataFrame({'y_test': y_test, 'y_pred': y_pred}), f'./pairplot_{model_name}.png')\n",
    "    plot_rel.plot_graph()\n",
    "    print(f\"Test accuracy -> cor: {pearsonr(y_pred, y_test)[0]:.4f}, mse: {np.mean((y_pred - y_test)**2):.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = list(numeric_feats)\n",
    "vars.append('SalePrice')\n",
    "plot_corration_map(pd.concat([df_features, df_target], axis=1)[vars])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance inflation factor, VIF, for one exogenous variable\n",
    "\n",
    "The variance inflation factor is a measure for the increase of the variance of the parameter estimates if an additional variable, is added to the linear regression. It is a measure for multicollinearity of the design matrix.\n",
    "\n",
    "One recommendation is that if VIF is greater than 5, then the explanatory variable is highly collinear with the other explanatory variables, and the parameter estimates will have large standard errors because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = numeric_feats\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(df_features[numeric_feats].values, i)\n",
    "                          for i in range(len(df_features[numeric_feats].columns))]\n",
    "  \n",
    "display(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = stats.shapiro(df_target['SalePrice'])\n",
    "print(f'H0: The data was drawn from a normal distribution. If pvalue > 0.05, we cannot reject the null hypothesis.')\n",
    "print(f'Shapiro Test: shapiro.statistic = {shapiro_test.statistic:.4f}, shapiro.pvalue = {shapiro_test.pvalue:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "sns.boxplot(x=df_target['SalePrice'], ax=ax)\n",
    "ax.grid(which = \"major\", axis='both', color='#758D99', zorder=1, linewidth = 0.5, alpha = 0.4,linestyle='-')\n",
    "ax.grid(which = \"minor\", axis='both', color='#758D99', zorder=1, linewidth = 0.3, alpha = 0.2,linestyle='-')\n",
    "ax.minorticks_on()\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, Random Forest demonstrates superior performance compared to Lasso. Despite our target data initially following a normal distribution, there is multicollinearity among our features. Consequently, employing Ordinary Least Squares (OLS) would not be a suitable option, even though the best Lasso model found has an alpha value of zero. Lasso effectively addresses multicollinearity through regularization.\n",
    "\n",
    "To enhance the accuracy of Lasso, we will explore a data transformation approach, once the skewness of target variable is equal 1.882876."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.loc[:,'LogSalePrice'] = np.log1p(df_target['SalePrice'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(df_target, 'LogSalePrice')\n",
    "shapiro_test = stats.shapiro(df_target['LogSalePrice'])\n",
    "print(f'H0: The data was drawn from a normal distribution. If pvalue > 0.05, we cannot reject the null hypothesis.')\n",
    "print(f'Shapiro Test: shapiro.statistic = {shapiro_test.statistic:.4f}, shapiro.pvalue = {shapiro_test.pvalue:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3))\n",
    "sns.boxplot(x=df_target['LogSalePrice'], ax=ax)\n",
    "ax.grid(which = \"major\", axis='both', color='#758D99', zorder=1, linewidth = 0.5, alpha = 0.4,linestyle='-')\n",
    "ax.grid(which = \"minor\", axis='both', color='#758D99', zorder=1, linewidth = 0.3, alpha = 0.2,linestyle='-')\n",
    "ax.minorticks_on()\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_target['LogSalePrice'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "y = df_target['LogSalePrice'].astype(float)\n",
    "X = df_features.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "dict_models = {\n",
    "'lasso': {\n",
    "    'model': Lasso(),\n",
    "    'space': [\n",
    "        Real(0, 0.02, name='alpha'),\n",
    "    ]\n",
    "},\n",
    "\n",
    "'rf': {\n",
    "    'space': [\n",
    "                Integer(100, 1000, name='n_estimators'),\n",
    "                Integer(2, 100, name='min_samples_split'),\n",
    "                Integer(1, 10, name='min_samples_leaf')\n",
    "                ],\n",
    "    'model': RandomForestRegressor()}\n",
    "}\n",
    "\n",
    "for model in dict_models:\n",
    "    model_name = model\n",
    "    space = dict_models[model]['space']\n",
    "    model = dict_models[model]['model']\n",
    "    optimizer = Optimizer(space=space, model=model,\n",
    "                            model_name=model_name, n_calls=20)\n",
    "\n",
    "    optimizer.find_optimal_params(X=X_train, y=y_train)\n",
    "    best_model = optimizer.best_model.fit(X_train, y_train.ravel())\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f\"Test accuracy -> cor: {pearsonr(np.expm1(y_pred), np.expm1(y_test))[0]:.4f}, mse: {np.mean((y_pred - y_test)**2):.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the skewness of our features and apply a transformation to specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_feats = df_features[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "display(skewness.sort_values(by=['Skew'], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_feats = df_features[numeric_feats].apply(\n",
    "    lambda x: skew(x.dropna()))  # compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "df_features[skewed_feats] = np.log1p(df_features[skewed_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skewed_feats = df_features[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "display(skewness.sort_values(by=['Skew'], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "y = df_target['LogSalePrice'].astype(float)\n",
    "X = df_features.reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "dict_models = {\n",
    "'lasso': {\n",
    "    'model': Lasso(),\n",
    "    'space': [\n",
    "        Real(0, 0.02, name='alpha'),\n",
    "    ]\n",
    "},\n",
    "\n",
    "'rf': {\n",
    "    'space': [\n",
    "                Integer(100, 1000, name='n_estimators'),\n",
    "                Integer(2, 100, name='min_samples_split'),\n",
    "                Integer(1, 10, name='min_samples_leaf')\n",
    "                ],\n",
    "    'model': RandomForestRegressor()}\n",
    "}\n",
    "\n",
    "for model in dict_models:\n",
    "    model_name = model\n",
    "    space = dict_models[model]['space']\n",
    "    model = dict_models[model]['model']\n",
    "    optimizer = Optimizer(space=space, model=model,\n",
    "                            model_name=model_name, n_calls=20)\n",
    "\n",
    "    optimizer.find_optimal_params(X=X_train, y=y_train)\n",
    "    best_model = optimizer.best_model.fit(X_train, y_train.ravel())\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(f\"Test accuracy -> cor: {pearsonr(np.expm1(y_pred), np.expm1(y_test))[0]:.4f}, mse: {np.mean((y_pred - y_test)**2):.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest outperformed Lasso and was minimally affected by data transformations, maintaining its accuracy. On the other hand, Lasso exhibited significantly lower performance when using the data in its original scale, with a correlation coefficient of 0.77, while Random Forest achieved a correlation coefficient of 0.92.\n",
    "\n",
    "However, after applying transformations to reduce the asymmetry of the target and features, Lasso showed improvement with a correlation coefficient of 0.84, while Random Forest maintained its high accuracy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
